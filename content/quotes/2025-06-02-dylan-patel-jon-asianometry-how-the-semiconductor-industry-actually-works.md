---
date: 2025-06-02
slug: "dylan-patel-jon-asianometry-how-the-semiconductor-industry-actually-works"
title: "Dylan Patel &amp; Jon (Asianometry) â€“ How the Semiconductor Industry Actually Works"
ref: https://share.snipd.com/episode/d0ce4e49-5f2a-4843-a7a9-2e805d289b78
tags:
  - quote
---

Quoting [Dwarkesh Podcast](https://share.snipd.com/episode/d0ce4e49-5f2a-4843-a7a9-2e805d289b78):

> **Semiconductor Industry Acceleration**

- Richard Chang left TSMC for China, accelerating their semiconductor industry. 
- Liang Mong Song, focused on pushing limits, later joined Samsung and then SMIC.

Transcript:
Dylan Patel
Guy that went to Samsung and SMIC and all that, I think you should tell that story.

Jon Y
There are two stories. There&#39;s a guy, he ran a semiconductor company in Taiwan called Worldwide Semiconductor. And this guy, Richard Chang, was very religious. I mean, all the TSMC people are pretty religious. But like he particularly was very fervent and he wanted to bring religion to China. So after he sold his company to TSMC, huge Cooper TSMC, he worked there for about eight or nine months. And he was like, all right, I&#39;ll go to China. Because back then, the relations between China and Taiwan were much more different. And so he goes over there. Shanghai says, we&#39;ll give you a bunch of money. And then Richard Chang basically recruits half of like a whole bunch. It&#39;s like a conga line of like Taiwanese line. Just like they get on the plane, they&#39;re flying over. And generally, that&#39;s actually a lot of like acceleration points within China&#39;s semiconductor industry. It&#39;s from talent flowing from Taiwan. And then the second thing was like Lian Mong Song. Lian Mong Song was a, is a nut. And I&#39;ve met him, I&#39;ve not met him. I&#39;ve met people who work with him and they say he is a nut. He is probably on the spectrum and he&#39;s, he does not care about people. He does not care about business. He does not care about anything. He wants to take it to the limit. The only thing, that&#39;s the only thing he cares about. He worked from TSMC, literal genius, 300 patents or whatever, 285. Goes, works all the way to like the top, top tier. And then one day he decides he loses out on some sort of power game within TSMC and gets demoted. And he was like head of R&amp;D, right, or something? He was like one of the top R&amp;D. He was like second or third place. And it was for the head of R&amp;D position, basically. Correct. For the head of R&amp;D position. He&#39;s like, I can&#39;t deal with this. And he goes to Samsung and he steals a whole bunch of talent from TSMC.

> **Semiconductor Research**

- Semiconductor research involves many experiments to improve recipes. 
- Researchers tweak tool parameters and measure yield, performance, and power.

Transcript:
Dylan Patel
I mean, I don&#39;t even know what to say about him.

Dwarkesh Patel
He&#39;s like 78 and he&#39;s like beyond brilliant, does not care about people. Like, yeah, what is research to make the next process node look like? Is it just a matter of like 100 researchers go in, they do like the next N plus one, then the next morning, the next 100 researchers go in? It&#39;s experiments. They have a recipe and what they do.

Jon Y
Every recipe, a TSMC recipe is the culminationation of a long, long years of research, right? It&#39;s highly secret. And the idea is that what you&#39;re going to do is that you go, you look at one particular part of it and you say, experiment, run an experiment. Is it better? Is it not? Is it better or not? Kind of a thing like that.

> **Decentralization of AI Efforts**

- The US AI landscape is decentralized, with multiple companies and startups. 
- China&#39;s AI efforts are also currently decentralized.

Transcript:
Dwarkesh Patel
Though. I guess we could start talking about NVIDIA. You know what? No, no, no. I think we should go back to China. There&#39;s like a lot of points there. All right. We covered the chips themselves. How do they get like the 10 gigawatt data center up?

Dylan Patel
What else do they need? I think there is a true like question of how decentralized do you go versus centralized, right? And if you look in the US, right, as far as like labs and such, the, you know, OpenAI, XAI, you know, Anthropic, and then Microsoft having their own effort, Anthropic having their own Efforts, despite having their partner, and then Meta.

> **Centralizing Compute in China**

- If Xi Jinping were scale-pilled, he should centralize compute resources. 
- This would address sanctions and maximize large-scale training potential.

Transcript:
Dylan Patel
And, you know, you go down the list, it&#39;s like there&#39;s a quite a decentralization uh and then all the startups like interesting startups that are out there doing stuff there&#39;s quite A decentralization of efforts uh today in china it is still quite decentralized right it&#39;s not like alibaba baidu you are the champions right you have like deep seek like who the hell Are you does government even support you like doing amazing stuff, right? If you are Xi Jinping and scale-pilled, you must now centralize the compute resources, right? Because you have sanctions on how many NVIDIA GPUs you can get in. Now, they&#39;re still north of a million a year, right? Even post-October last year sanctions. We still have more than a million H20s and other Hopper GPUs getting in through other means, but legally like the H20s. And then on top of that, you have your domestic chips, right? But that&#39;s less than a million chips. So then when you look at it, it&#39;s like, oh, well, we&#39;re still talking about a million chips. The scale of data centers people are training on today slash over the next six months is 100,000 GPUs, right? OpenAI, XAI, right. These are like quite well documented and others. But in China, they have no individual system of that scale yet. Right. So then the question is like, how do we get there? You know, no company has had the centralization push to have a cluster that large and train on it yet, at least publicly like well-known. And the best models seem to be from a company that has got like 10,000 GPUs, right? Or 16,000 GPUs, right? So it&#39;s not quite as centralized as the US companies are. And the US companies are quite decentralized. If you&#39;re Xi Jinping and you&#39;re scale-pilled, do you just say XYZ company is now in charge and every GPU goes to one place.

> **Hiding Compute**

- China could easily hide its AI compute capabilities due to the sheer scale of manufacturing and energy-intensive industries already present.
- Converting an aluminum mill to AI compute would be hard to detect.

Transcript:
Dylan Patel
You know, actually, I think, the other thing is like, could we notice it? I don&#39;t think so, because the amount of like factories that are being spun up, the amount of other construction, manufacturing, et cetera, that&#39;s being built, a gigawatt is actually Like a drop in the bucket, right? Like a gigawatt is not a lot of power. 10 gigawatts is not an absurd amount of power, right? It&#39;s okay. Yes. It&#39;s like hundreds of thousands of homes, right? Well, yeah. Millions of people, but it&#39;s like, you got 1.4 billion people. You got like most of the world&#39;s like extremely energy intensive, like refining and like, you know, rare earth refining and all these manufacturing industries are here. It would be very easy to hide it. It&#39;d be very easy to just like shut down. Like I think the largest aluminum mill in the world is there. And it&#39;s like, it&#39;s like North of five gigawatts alone. It&#39;s like, Oh, what could we tell if they stopped making aluminum there and instead started like making, you know, AIs there or making AI there? Like, I don&#39;t know if we could tell, right? Because they could also just easily spawn like 10 other aluminum mills, make up for the production and be fine, right? So like there&#39;s many ways for them to hide compute as well. To the

> **US-China Tech Landscape**

- China excels at physical infrastructure for AI, but the West dominates chip manufacturing. 
- Advanced packaging capacity is hard for the US to sanction effectively.

Transcript:
Dwarkesh Patel
You could just take out a five gigawatt aluminum refining center and like build a giant data center there, then I guess the way to control Chinese AI has to be the chips because like everything Else, so like how do you like, just like walk me through how many chips do they have now? How many will they have in the future? What will they like? How many is that in comparison to US and the rest of the world?

Dylan Patel
Yeah. So in the world, I mean, the world we live in is they are not restricted at all in like the physical infrastructure side of things in terms of power, data centers, etc. Because their supply chain is built for that, right? And it&#39;s pretty easy to pivot that. Whereas the US adds so little power each year, and Europe loses power every year, the Western sort of industry for power is non-existent in comparison, right? But on the flip side is, quote unquote, Western, including Taiwan, chip manufacturing is way, way, way, way, way larger than China&#39;s, especially on leading edge, where China theoretically Has, you know, depending on the way you look at it, either zero or a very small percentage share, right? And so there you have, you have wafer, you have, you have equipment, wafer manufacturing, and then you have advanced packaging capacity, right? And where the U.S. Can control China, right? So advanced packaging capacity is kind of a shot because the vast majority, the largest advanced packaging company in the world was Hong Kong headquartered. They just moved to Singapore, but like that&#39;s effectively like, you know, in a realm where the US can&#39;t sanction it, right? A majority of these other companies are in similar places, right? So advanced packaging capacity is very hard, right? Advanced packaging is useful for stacking memory, stacking chips on co-auths, right? Things like that. And then the step down is wafer fabrication. There is immense capability to restrict China there. And despite the US making some sanctions, China in the most recent quarters was like 48% of ASML&#39;s revenue, right? So, you know, and like 45% of like applied materials and you just go down the list. So it&#39;s like, obviously it&#39;s not being controlled that effectively, but it could be on the equipment side of things. The chip side of things is actually being controlled quite effectively, I think, right?

> **Ineffectiveness of Export Controls**

- Dylan Patel believes export controls are ineffective at stopping China&#39;s progress. 
- Each sanction motivates China to develop domestic alternatives.

Transcript:
Dwarkesh Patel
John dylan seems to think the expert controls are kind of a failure do you do you agree with them or that is a very interesting question because i think it&#39;s like.

Jon Y
Why, thank you. Like, what do you.

Dylan Patel
Darkish, you&#39;re so good. Yeah, Darkish, you&#39;re the best.

Jon Y
I think failure is a tough word to say because I think it&#39;s like, what are we trying to achieve, right? Like, they&#39;re talking about AI, right? Yeah. When you do sanctions like that, you such a deep knowledge of the technologies. Just taking lithography, right?

Dylan Patel
If your goal is to restrict China from building chips and you just boil it down to like, hey, lithography is 30% of making a chip or 25%. Cool. Let&#39;s sanction lithography. OK, where do we draw the line? OK, let me ask. Let me ask. Let me figure out where the line is. And if I&#39;m a bureaucratrat if i&#39;m a lawyer at the commerce department or what have you well obviously i&#39;m going to go talk to asml and asml is going to tell me this is the line because they Know like hey well you know this this this is you know there&#39;s like some blending over there&#39;s like they&#39;re they&#39;re like looking at like what&#39;s going to cost us the most money right and Then they constantly say like if you restrict us then china will have their own industry right and and the way I like to look at it is like chip manufacturing is like like 3D chess or like, You know, a massive jigsaw puzzle in that if you take away one piece, China can be like, oh, that&#39;s the piece. Let&#39;s put it in. Right. Year by year by year, they keep updating them ever since like 2018 or so 19, right? When Trump started and now Biden&#39;s, you know, accelerated them. They&#39;ve been like, they haven&#39;t just like take a bat to the table and like break it. Right. Like, it&#39;s like, let&#39;s take one jigsaw puzzle out, walk away. Oh shit. Let&#39;s take two more out. Oh shit. Right. Like, you know, it&#39;s like, instead if they like, they, you either have to go kind of like full bat to the fricking like table slash wall or, or chill out, right? Like, and like, you know, let them let them do whatever they want. Because the alternative is everything is focused on this thing and they make that. And then now when you take out another two pieces, like, well, I have my domestic industry for this. I can also now

> **SMIC&#39;s Expansion**

- SMIC is building 7nm capacity in Beijing and 5nm in Beijing. 
- They claim the tools imported for this are for 28nm production.

Transcript:
Dylan Patel
Oh, the reason why I was bringing up Shanghai, they&#39;re building seven nanometer capacity in Beijing. They&#39;re building five nanometer capacity in Beijing, but the U.S. Government doesn&#39;t care. And they&#39;re importing dozens of tools into Beijing. And they&#39;re saying to the U.S. Government and ASML, this is for 28 nanometer, obviously. Right. This is not bad. And then obviously, you know, like in the background, yeah, we&#39;re making five nanometer here.

Dwarkesh Patel
Are they doing it because they believe in AI or because they want to make Huawei phones?

> **Huawei&#39;s Culture**

- Huawei&#39;s success stems from its intense, almost military-like culture. 
- This &#34;struggle&#34; culture and paranoia drive constant innovation.

Transcript:
Dylan Patel
The military, because it&#39;s the PLA.

Jon Y
It is, it is generally seen as an arm of the PLA. But like, how do you square that with the fact that sometimes the PLA seems to mess stuff up?

Dylan Patel
Oh, like filling water in rockets?

Jon Y
I don&#39;t know if that was true.

Dylan Patel
I&#39;m not denying it. There is like that crazy conspiracy. Not conspiracy. You don&#39;t know what the hell to believe in China, especially as a not Chinese person. Nobody knows.

Jon Y
Even Chinese people don&#39;t know what&#39;s going on in China.

Dylan Patel
There&#39;s like, you know, like all sorts of stuff like, oh, they&#39;re filling water in their rockets. Clearly they&#39;re like incompetent. It&#39;s like, look, if I&#39;m the Chinese military, I want the Western world to like believe I&#39;m completely incompetent. Because one day I can just like destroy the fuck out of everything. Right. With all these hypersonic missiles and all this shit. Right. Like drones and like, no, no, no, no. We&#39;re filling water in our missiles. These are all fake. We don&#39;t actually have a hundred thousand missiles that we manufacture in a facility that&#39;s like super hyper advanced and Raytheon is stupid as shit because they can&#39;t make, you know, Missiles nearly as fast, right? Like, I think like, that&#39;s also like a flip side is like how much false propaganda is there, right?

> **Vertical Integration**

- The semiconductor industry is stratified, with few competitors in each layer. 
- Companies accepting external products improved faster than vertically integrated ones.

Transcript:
Dwarkesh Patel
Isn&#39;t there more vertical integration in the semiconductor industry? Well, like why are there like this subcomponent requires this other subcomponent from this other company, which requires a subcomponent from another company? Why is more of it not done in-house?

Dylan Patel
The way to look at it today is it&#39;s super, super stratified. And every industry has anywhere from one to three competitors. And pretty much the most competitive it gets is like 70% share, 25% share, 5% share in any layer of like manufacturing chips, anything, anything, chemicals, different types of chips. But it used to be vertically integrated.

Jon Y
Or the very beginning it was integrated, right? Where did that stop? What happened was the funniest thing was like, you know, you had companies that used to do it all in the one. And then suddenly sometimes a guy would be like, I hate this. I think I know how to do better. Spins off, does his own thing, starts his company, goes back to his old company, says, I can sell you a product that&#39;s better, right? And that&#39;s the beginning of what we call the semiconductor manufacturing equipment industry. Like basically- Like in the 70s, right? Like everyone made their own equipment. 60s and 70s, like you spin off all these people. And then what happened was that the companies that accepted these outside products and equipment got better stuff.

> **Foundry Consolidation**

- Foundries consolidate as leading-edge development costs rise. 
- N2&#39;s existence is economically questionable without concentrated spending by a few key players.

Transcript:
Dwarkesh Patel
Fewer of them every year, right? So there&#39;s maybe more companies overall, but the final people who make the wafers, there&#39;s less and less. And then it&#39;s interesting in a way, it&#39;s similar to the AI foundation models where you need to use the revenues from a previous model or your market share to fund the next round of ever More expensive development. When TSMC launched the foundry industry, right?

Jon Y
And when they started, there was a whole wave of like Asian companies that funded semiconductor foundries of their own. You had Malaysia with Sotera. You have Singapore with Chartered. You had, there was a lot of worldwide, there&#39;s a wide semiconductor where I talked about earlier. There&#39;s one from Hong Kong. Bunch in Japan. Bunch in Japan. They all sort of did this thing, right? And I think the thing was that when you&#39;re going to leading edge, when the thing is that, it got harder and harder, which means that you had to aggregate more demand from all the customers To fund the next node, right? So technically, in the sense that what it&#39;s kind of do is aggregating all this money, all this profit to kind of fund this next node to the point where now, like, there&#39;s no room in the market For an N2 or N3. Like, technically, you could argue that economically, you can make an argument that, like, N2 is a monstrosity that doesn&#39;t make sense economically, which should not exist in some Ways without the immense single concentrated spend of like five players in the market.

> **Taiwan Shutdown Impact**

- A Taiwan shutdown would crash markets and disrupt global tech and manufacturing. 
- Cars, appliances, and everyday electronics rely heavily on Taiwanese chips.

Transcript:
Dwarkesh Patel
I want to ask the normie question for like everybody&#39;s world. I want to phrase it that way. Okay. I want to ask a question that&#39;s like a normie.

Dylan Patel
Not for you nerds. I think, I think John and I could communicate to, to like the point where you wouldn&#39;t even know what the fuck. Okay.

Dwarkesh Patel
Suppose Taiwan is invaded or Taiwan has an earthquake nothing is shipped out of taiwan and from now on what happens next the rest of the world how would it feel its impact a day in a weekend A month in a year in i mean it&#39;s it&#39;s a terrible thing it&#39;s a terrible thing to talk about i think it&#39;s like can you just say it&#39;s all terrible everything&#39;s terrible because it&#39;s not just

Jon Y
Like leading at leading edgeading edge, people will focus on leading edge. But there&#39;s a lot of trailing edge stuff that like people depend on every day. I mean, we all worry about AI. The reality is you&#39;re not gonna get your fridge. You&#39;re not gonna get your cars. You&#39;re not gonna get everything. It&#39;s terrible. And then there&#39;s the human part of it, right? It&#39;s all terrible. Can we, like,

> 1min Snip

Transcript:
Dwarkesh Patel
Suppose Taiwan is invaded or Taiwan has an earthquake nothing is shipped out of taiwan and from now on what happens next the rest of the world how would it feel its impact a day in a weekend A month in a year in i mean it&#39;s it&#39;s a terrible thing it&#39;s a terrible thing to talk about i think it&#39;s like can you just say it&#39;s all terrible everything&#39;s terrible because it&#39;s not just

Jon Y
Like leading at leading edgeading edge, people will focus on leading edge. But there&#39;s a lot of trailing edge stuff that like people depend on every day. I mean, we all worry about AI. The reality is you&#39;re not gonna get your fridge. You&#39;re not gonna get your cars. You&#39;re not gonna get everything. It&#39;s terrible. And then there&#39;s the human part of it, right? It&#39;s all terrible. Can we, like, it&#39;s depressing.

Dylan Patel
I think. And I live there. Yeah. I think day one market crashes a lot, right? You got to think about like, I think the big six biggest companies, Magnificent Seven, whatever the heck it&#39;s called, are like 60, 75% of the S&amp;P 500 and their entire business relies On chips, right? Google, Microsoft, Apple, NVIDIA, you know, you go down the list, right?

> **Chip Dependence in Cars**

- If Taiwan is invaded, the supply of chips to make new cars would be gone in six months. 
- Dylan Patel mentions that modern cars have thousands of chips, including four in every Tesla door handle.

Transcript:
Dylan Patel
But like the supply chain is trying to like figure out what the hell to do to refix it. But six months in the supply of chips for making new cars gone or sequestered to make military shit. Right. Um, you can no longer make cars. Um, and we don&#39;t even know how to make non semiconductor induced cars, right? Like this unholy concoction with all these like chips, right? Cars like 40% chips now. Like it&#39;s just chips in the tires. There&#39;s like 2,000 plus chips. Every Tesla door handle has like four chips in it. It&#39;s like, what the fuck?

> **Semiconductors vs. AI**

- Semiconductor expertise requires extensive formal education, unlike AI. In AI, prodigies can emerge and quickly contribute without the same level of traditional schooling.
- Dylan Patel contrasts this with the semiconductor industry, where specialization and formal qualifications are essential.

Transcript:
Dwarkesh Patel
There are like 18 year olds who are just cracked at AI, right?

Dylan Patel
Already, right? And like there&#39;s high school dropouts that get like jobs at OpenAI. This existed in the past, right? Pat Galsinger, current CEO of Intel, went straight to work. He was, he like grew up in the Amish area of Pennsylvania and he went straight to work at Intel, right? Because he&#39;s just cracked, right? That is not possible in semiconductors today. You can&#39;t even get like a job at like a tool company without like a, at least like a freaking master&#39;s in chemistry, right? And probably a PhD, right? Like, like of the like 75,000 TSMC workers, it&#39;s like 50,000 have a PhD or something insane, right? It&#39;s like, okay, this is like, there&#39;s like some, there&#39;s like a next level amount of like how specialized everything&#39;s gotten. Whereas today, like you can take like, you know, Sholto, you know, he, when did he start working on AI? Not that long ago. Not to say anything bad about Sholto. No, no, no, but he&#39;s cracked. He&#39;s like Omega cracked at like what he does. What he does, you could pick them up and drop them into another part of the AI stack. First of all, he understands it already. And then second of all, he could probably become cracked at that too, right? Whereas that is not the case in semiconductors, right? One, you specialize like crazy. Two, you can&#39;t just pick it up. You know, like Shulte, I think, what did he say?

Dwarkesh Patel
He just started like... He was a consultant in McKinsey. And at night, he would read papers about robotics and run experiments and whatever.

> **Siloed Semiconductor Knowledge**

- Semiconductor knowledge is highly compartmentalized, unlike AI where state-of-the-art information is publicly accessible.
-  Tool vendors don&#39;t fully know what Intel and TSMC do, and vice versa, highlighting manufacturing stratification.

Transcript:
Dylan Patel
That, and like, what is state of the art is public.

Jon Y
That is not the case in semiconductors. Semiconductors have been shut down since the 1960s, 1970s, basically. I mean, like, it&#39;s kind of crazy how little information has been formally transmitted from one country to another. Like, the last time you could really think of this was like 19, maybe the Samsung era, right? So then how do you guys keep up with it? Well, we don&#39;t know it. I don&#39;t personally. I don&#39;t think I know it. I don&#39;t, I mean, I. If you don&#39;t know it, what are you making videos about? It&#39;s crazy because, like, there&#39;s a guy. There&#39;s like, I spoke to one guy, he&#39;s like a PhD in etch or something. The world, one of the top people in etch. And he&#39;s like, man, you really know like lithography, right? I&#39;m just like, I don&#39;t feel like I know lithography. But then you&#39;ve talked to people who know lithography, you&#39;ve done pretty good work in packaging, right? Nobody knows anything.

Dwarkesh Patel
They all have gel man amnesia.

Jon Y
They&#39;re all in this like single well, right? They&#39;re digging deep. They&#39;re digging deep for what they&#39;re getting at, but they, but you know, they don&#39;t know the other stuff well enough.

> **Industry Coordination**

- Semiconductor industry coordination resembles gossip and coalescence around ideas. 
- Gordon Moore&#39;s observation became a driving force, shaping engineering goals.

Transcript:
Dwarkesh Patel
It too. So if that&#39;s the case, if like nobody knows the whole stack then how does the industry coordinate to be like um uh you know in five in two years we want to go to the next process which has gate All around and for that we need x tools the next technology is developed by whatever that&#39;s really fascinating it&#39;s a fascinating social kind of phenomenon right you can feel it i went

Jon Y
To europe the earlier this year d was like, had allergies. But like, I was like, talking to those other people. And you can just, it&#39;s like gossip. It&#39;s gossip. You start feeling the, you start feeling people coalescing around like a something, right? Early on, we used to have like Semitech where people, all these American companies came together and talked and they came and they hammered out, right? But Semitech in reality was dominated by a single company, right? But then, you know, nowadays it&#39;s a little more dispersed, right? You feel like it&#39;s like it&#39;s like it&#39;s a blue moon arising kind of thing. Like they are going towards something. They know it. And then suddenly the whole industry is like, this is it. Let&#39;s do it.

Dylan Patel
I think it&#39;s like God came and proclaimed it. Will shrink density 2x every two years gordon morse he made an observation and then like it didn&#39;t go nowhere it went way further than he ever expected because it&#39;s like oh there&#39;s line Of sight to get to here and here and like and he predicted like seven eight years out like multiple orders of magnitude of increases in transistors and it came true But then by then the Entire industry was like, this is obviously true.

> **Siloed Semiconductor Knowledge**

- Semiconductor knowledge is highly siloed within companies and passed down through master-apprentice relationships.
- AI can&#39;t easily learn about semiconductors due to the undocumented nature and reliance on human intuition.

Transcript:
Dylan Patel
And the data and knowledge within each layer is A not documented online at all right documentation because it&#39;s all siloed within companies um b it is there&#39;s a lot of human element to It because a lot of the knowledge like as john was saying is like apprentice master apprentice master type of uh knowledge or i&#39;ve been doing this for 30 years and there&#39;s an an amazing Amount of intuition on what to do just when you see something, um, to where like, AI can&#39;t just learn semiconductors like that. But at the same time, there&#39;s a massive amount of talent shortage and ability to move forward on things. Right. So like the technology used on like, like most of the like equipment in semiconductor tool fabs runs on like windows xp right like each tool has like a windows xp server on it or like you Know like all the chip design tools like have like centos centos like version six right and like that&#39;s old as hell right so like there&#39;s like so many like areas where like why is this so Far behind at the same time it&#39;s like so like hyper optimized that&#39;s like

> **Semiconductor Complexity**

- Dylan Patel states that semiconductor manufacturing and design represent the largest search space of any human endeavor due to its extreme complexity.
- Leading-edge chips have hundreds of billions of transistors, each with many permutations.

Transcript:
Dylan Patel
I think, I think, you know, it&#39;s first, it&#39;s important to state that semiconductor manufacturing and design is the largest search space of any problem that humans do because it is the Most complicated industry that anything that humans do. And so, you

> **Hardware&#39;s Influence on Model Architecture**

- Hardware influences optimal model architecture, leading to divergence between companies. 
- Chinese models might differ due to hardware constraints and data/use case differences.

Transcript:
Dwarkesh Patel
Like two narratives you can tell here of how this happens. One is that these AI companies who are training the foundation models, who understand the tradeoffs of like how much is the marginal increase in compute versus memory worth to them And what tradeoffs do they want between different kinds of memory. They understand this. In a way that&#39;s like most optimal or and also design like the architecture of the the model itself in a way that uh uh uh reflects like what are the hardware trade-offs another is nvidia Because it has like i i don&#39;t know how this works presumably they have some sort of like know-how like they&#39;re accumulating all this like uh knowledge about how to better design this Architecture and like also better search tools for so on. Who has basically like better moat here in terms of, will NVIDIA keep getting better at design, getting this 100x improvement or will it be like OpenAI and Microsoft and Amazon and Anthropic Who are designing their accelerators who will keep getting better at like designing the accelerator?

Dylan Patel
I think that there&#39;s a few vectors to go here, right? One is you mentioned, and I think it&#39;s important to note, is that hardware has a huge influence on the model architecture that&#39;s optimal. And so it&#39;s not a one-way street that better chip equals, you know, the optimal model for Google to run on TPUs, given a given amount of dollars, a given amount of compute, is different Architecturally than what it is for OpenAI with NVIDIA stuff, right? It is like absolutely different. And then like, even down to like networking decisions that different companies do and data center design decisions that people do, the optimal, like if you were to say, you know, X amount Of compute of TPU versus GPU, compute optimally, what is the best thing?

> **Building an AI Cluster**

- As head of compute, prioritize building a large, single-site cluster. 
- Look for existing infrastructure like Bitcoin mining sites for faster deployment.

Transcript:
Dwarkesh Patel
You are head of compute. Help us get on the map. We&#39;re going to compete with the frontier labs. What is your first step?

Dylan Patel
Okay. So the constraints are you&#39;re a US slash Israeli firm because that&#39;s what SSI is, right? And your researchers are in the US and Israel. You probably can&#39;t build data centers in Israel because power is expensive as hell. And it&#39;s probably like risky, maybe. I don&#39;t know. So still in the US, most likely. Most of the researchers are here. So or a lot of them are in the US, right? Like Palo Alto or whatever. So I guess you need a significant chunk of compute. Obviously, though, like the whole pitch is you&#39;re going to make some research breakthrough. That&#39;s like compute efficiency win, data efficiency win, whatever it is. You&#39;re going to make some breakthrough, but you need compute to get there, right? Because your GPUs per researcher is your research velocity, right? Obviously, like data centers are very tapped out, right? Not in terms of tapped out, but like every new data center that&#39;s coming up, most of them have been sold, which has led people like Elon to go through this, like, insane thing in Memphis, Right? I&#39;m just trying to, like, I&#39;m just trying to square the circle, yeah.

Dwarkesh Patel
On that question, I kid you not, in my group house, like, group chat, like, there have been two separate people who have been, like, I have a cluster of H100s and I have a long lease on them, But I&#39;m trying to sell them off. Is it like a buyer&#39;s market right now? Because it does seem like people are trying to get rid of them.

Dylan Patel
So I think for the ILIA question, a cluster of 256 GPUs or even 4K GPUs is kind of cope, right? It&#39;s not enough, right? Yes, you&#39;re going to make compute efficiency wins, but with a billion dollars, you probably just want the biggest cluster in one individual spot. And so like small amounts of GPUs, probably not like, you know, possible to use, right? Like for them, right? Like, and that&#39;s what most of the sales are, right? Like you go and look at like GPU list or like Vast or like foundry, like, or a hundred different GPU resellers, the cluster sizes are small. Now, is it a, is it a buyer&#39;s market? Yeah. Last year you would buy H one hundreds for like $4 or $3. Like if you, you know, an hour, an hour, right. For shorter term or midterm deals right now, it&#39;s like, if you want a six month deal, you can get like $2, 15 cents or less. Right. Like, and like the natural cost, if I have a data center, right? And I&#39;m paying like standard data center pricing to purchase the GPUs and deploy them is like $1.40. And then you add on the debt, because I probably took debt to buy the GPUs or cost equity, cost of capital gets up to like $1.70 or something, right? And so you see deals that are like the good deals, right? Like Microsoft renting from CoreWeaver, like $1.90 to $2, right? So people are getting closer and closer to like, there&#39;s still a lot of profit, right? Because the natural rate, even after debt and all this is like $1.70. So like there&#39;s still a lot of profit when people are selling in the low twos, like GPU companies, people deploying them. But it is a buyer&#39;s market in a sense that it&#39;s gotten a lot cheaper. But cost of compute is going to continue to tank, right? Because it&#39;s like sort of like, I don&#39;t remember the exact name of the law, but it&#39;s effectively Moore&#39;s law, right? Every two years, the cost of transistors halved, and yet the industry grew, right? Every six months or three months, the cost of intelligence, you know, like OpenAI and GBD, GBD4, what, February 2023, right? $120 per million tokens or something like that was roughly the cost. And now it&#39;s like 10, right? It&#39;s like, it&#39;s like the cost of intelligence is tanking partially because of compute, partially because the model&#39;s compute efficiency wins, right? I think that&#39;s a trend we&#39;ll see. And then that&#39;s going to drive adoption as you scale up and get, make it cheaper and scale up and make it cheaper. Right, right, right.

> **Future of Compute Scaling**

- By 2026, single-site gigawatt clusters will exist, but multi-site scaling is key. 
- Pre-training flops are less relevant than total compute across training/inference.

Transcript:
Dwarkesh Patel
So, let&#39;s just like give the number on like, okay, 2025 Elon&#39;s cluster is going to be the big, like, it doesn&#39;t matter who it is.

Dylan Patel
So, so then there&#39;s the definition game, right? Like Elon claims he has the largest cluster at a hundred K GPUs because they&#39;re all fully connected. Rather than who it is.

Dwarkesh Patel
Like, I just want to know, like how, how many, like, I don&#39;t know if it&#39;s better to denominate and use this year. Okay. Right. For the biggest cluster. For the biggest cluster.

Dylan Patel
Next year? Next year, 300,000 to 500,000, depending on whether it&#39;s one side or many, right? 300,000 to, like, 700,000, I think, is the upper bound of that. But anyways, like, you know, it&#39;s about, like, when they tier it on, when they can connect them, when the fibers connect it together. Anyways, 300 to like 500,000, let&#39;s say. But those GPUs are 2 to 3x faster, right? Versus the 100K cluster. So on an H100 equivalent basis, you&#39;re at a million chips next year. In one cluster? By the end of the year, yes. No, no, no. Well, so one cluster is like the wishy-washy definition, right? Multi-site, right? Can you do multi-site? What&#39;s the efficiency loss when you go multi-site? Is it possible at all? I truly believe so. What&#39;s the efficiency loss is the question, right? Okay, would it be like 20% loss, 50% loss? Great question. This is where you need the secrets, right? And Anthropix got similar plans with Amazon, and you go down the list, right? And then the year after that? The year after that is where. This is 2026. 2026, there is a single gigawatt site. And that&#39;s just part of the like multiple sites, right? For Microsoft. The Microsoft five gigawatt thing happens in 20. One gigawatt, one site in 2026. But then you have, you know, a number of others. You have five different locations, each with multiple, some with multiple sites, some with single site. You&#39;re easily north of two, three gigawatts. And then the question is, can you start using the old chips with the new chips? And like the scaling, I think is like, you&#39;re going to continue to see flop scaling like much faster than people expect. I think as long as the money pours in, right? Like that&#39;s the other thing is like, there&#39;s no fucking way you can pay for the scale of clusters that are being planned to be built next year for OpenAI unless they raise like 50 to $100 Billion, which I think they will raise that like end of this year, early next year. 50 to 100 billion? Yes. Are you kidding me? No. Oh my God. This is like, you know, like Sam has a superpower, no? Like, it&#39;s like recruiting and, like, raising money. That&#39;s, like, what he&#39;s, like, a god at. Will ships themselves be a bottleneck to the scaling? Not in the near term. It&#39;s more, again, back to the concentration versus decentralization point. Yeah, yeah. Because, like, the largest cluster is 100,000 GPUs. NVIDIA is manufactured close to 6 million hoppers, right, across last year and this year, right? So, like, what? That&#39;s fucking tiny, right?

Dwarkesh Patel
So then why is Sam talking about a 7 trillion to build foundries and whatever?

Dylan Patel
Well, this is, you know, like, draw the line, right? Like, log, log lines. Let&#39;s fuck, number goes up, right? You know, if you do that, right? Like, you&#39;re going from 100K to 300 to 500K, where the equivalent is a million, you just 10X year on year. Do that again. Do that again, or more, right? If you increase the pacing, what is do that again? So like 2026, like the number of H100 equivalents, you know, if you increase the globally produced flops by like 30 X year on year or 10 X year on year, and the cluster size grows or the cluster Size grows by, you know, three to five to seven X. And then you do your start, you get multi-site going better and better and better. You can get to the point where multi-million chip clusters, I either can, even if they&#39;re like regionally not connected right next to each other, are right there. And in terms of flops, like it would be 1E what? 1E28, 29? I think 1E30 is like very possible, like 28, 29.

Dwarkesh Patel
Wow. Yeah. And 1E30, you said by 28, 29? Yeah. And so that is literally six orders of magnitude. That&#39;s like 100,000 times more compute than GPT-4. The other thing to say is like the way you count flops on a training run is really stupid.

Dylan Patel
Like you can&#39;t just do like active parameters times tokens times six, right? Like that&#39;s really dumb because like the paradigm, as you mentioned, right, is like, and you&#39;ve had many great podcasts on this, like synthetic data and like RL stuff, post-training, Like verifying data and like all these things generating and throwing it away, like all sorts of stuff, search, like inference time compute, all these things like aren&#39;t counted in The training flops. So you can&#39;t say, 1A30 is a really stupid number to say because by then the actual flops of the pre-training may be X, but the data to generate for the pre-training may be way bigger or the Search inference time may be way, way bigger, right? Right. But also, because
