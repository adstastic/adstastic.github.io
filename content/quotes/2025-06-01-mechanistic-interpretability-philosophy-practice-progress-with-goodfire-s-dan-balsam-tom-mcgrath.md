---
date: 2025-06-01
slug: "mechanistic-interpretability-philosophy-practice-progress-with-goodfire-s-dan-balsam-tom-mcgrath"
title: "Mechanistic Interpretability: Philosophy, Practice &amp; Progress With Goodfire&#39;s Dan Balsam &amp; Tom McGrath"
ref: https://share.snipd.com/episode/37f92fa4-41ee-4ee0-b533-ea160bb7e8cd
tags:
  - quote
---

Quoting [&#34;The Cognitive Revolution&#34; | AI Builders, Researchers, and Live Player Analysis](https://share.snipd.com/episode/37f92fa4-41ee-4ee0-b533-ea160bb7e8cd):

> **Interpretability as Empirical Science**

- Interpretability relies heavily on rich empirical data from models&#39; internal activations.
- Progress is like natural science: observing phenomena and forming hypotheses gradually.

Transcript:
Nathan Labenz
This data compute and algorithms paradigm. These are the three legs of the stool that are enabling progress. There&#39;s the sense that they all kind of contribute equally. And then on the interpretability side, I&#39;m sort of tempted to slot in models for data and say like models compute and algorithms are maybe the things. And seemingly a lot depends on the quality of the models. But then there&#39;s still a role for data. So how do you guys think of the kind of fundamental inputs of what you&#39;re doing?

Tom McGrath
I think one thing that&#39;s interesting here is that there&#39;s... These inputs are very important, right? And the models have changed, and so things sort of changed along with them. But another thing is, it&#39;s a very empirical activity, in the sense of you&#39;re looking in a fairly fine-grained way at the data of what&#39;s happening inside models. So progress in algorithms, for instance, parameter decomposition, or or whatever, we can get into these later. But one of the inputs here is actually like very rich empirical data. You know, when you&#39;re just like, look, basically, like, what are activations like when we like sort of tinker around with models in a relatively hypothesis free way? Do they actually seem to behave? And I guess like this sort of of empirical input is always part of progress in algorithms, but I think I just want to upweight it for interpretability because we&#39;re really doing this Sort of natural science. But yeah, I broadly agree with the decomposition of models of the data, I suppose, and then there&#39;s compute. And I think as a field, we would like to be able to use much more compute. And then there&#39;s algorithms. And I think that&#39;s a good decomposition. I would say that we don&#39;t have like the transformer, but for interpretability. And it could be that the thing that we are blocked on for finding the transformer for interpretability is simply understanding models better in a way that we can like then generalize To a new inductive bias.

Nathan Labenz
Yeah, that&#39;s really interesting and you kind of prompted me also to try another paradigm mapping exercise you said we&#39;re doing this in a relatively hypothesis free way that sort of Maps in my mind to like pre-training sort of unsupervised learning right the sort of sae paradigm has largely been run a ton of data through basically from the same original data set. You can tell me more about how that&#39;s curated, maybe especially for interpretability work, but relatively hypothesis free way. Is there any equivalent to like post training yet in the interpretability world?

Tom McGrath
Ah, so when I say that&#39;s interesting, when I say hypothesis-free, I mainly mean like a person sitting down and tinkering with models and being like, what structure is there? So for instance, like this kind of famous, not all language model features a linear paper. They&#39;re sort of finding structure in activations. And this structure is kind of interesting. It&#39;s this kind of higher dimensional manifold. And you don&#39;t necessarily get to this by being like, I have a hypothesis that things are this way. You just sort of like a natural scientist, like someone out observing the stars or something. They kind of, they have some hypotheses in mind, but they&#39;re also just going, they&#39;re just sort of looking at things. And so that&#39;s what I mean. But it&#39;s interesting that actually I also am very keen on the unsupervised to interpret models like SAEs for basically the same reason, except that every architecture is kind of coming With a hypothesis like the SAE inductive biases. The SAE is like a fairly strong hypothesis that features are literally directions in embedding space.

Nathan Labenz
Yeah, there&#39;s something really interesting there around, and again, the model quality seems to be really important, right? Because a lot of the kind of hypothesis-free tinkering, sort of even vibe coding, right? I mean, Neil Nando recently put out a video of him doing some vibe coding research, and that&#39;s all kind of premised on the idea that you can run a little experiment and get something back Relatively quickly on a sort of iteration timeline where ideally you could sustain focus. Maybe you have to go take a walk and come back. But it&#39;s not like really long time frames or really large compute budgets. Whereas, as you mentioned, like you got to have some conviction to throw large amounts of compute at an SAE at scale. But I wonder, like, how do you think about the sort of challenge of, can you do that like rapid experimentation on like the truly large advanced models? Or are you limited to working with something like GPT-2 scale? And does that sort of create like a fundamentally different regime from the things that you end up scaling up?

Tom McGrath
It&#39;s really a question of infrastructure. If you have no infrastructure, then it&#39;s hard. If you have the right infrastructure, then building that infrastructure is hard, but the right infrastructure makes experiments relatively easy.

Dan Balsam
Yeah, the other thing that I&#39;ll add is when thinking about unsupervised techniques as hypothesis generators for how the model could be working, there&#39;s no way that we&#39;re going to be Able to scale to superintelligence without making our interpretability techniques unsupervised. And that&#39;s one of the things that really motivates us is our mission at Good Fire. Narrowly, superintelligent models already exist in the scientific domains, and this is what we spend a bunch of our time working on. You know, when you&#39;re working with a genomics model, you&#39;re working with a model that we have priors about. There&#39;s lots of bioinformatic research to attempt to understand the genome, but we&#39;re also working with systems for which, you know, our statistical techniques that we have are not As explanatory as we&#39;d like to be. And that&#39;s the motivation towards moving towards AI to begin with. And so in the process of training on supervised models, it really gives us a grounding about where to look in the model to begin with. It gives us a bunch of candidate experiments,

> **Sparse Autoencoders as Microscopes**

- Sparse autoencoders (SAEs) give a reductive sensor into model internals with reconstruction loss trade-offs.
- Improving SAEs and developing experiment scaffolding is key to better model abstraction and interpretability.

Transcript:
Nathan Labenz
This question of fundamental units. And this is sort of a philosophical question I&#39;ve been trying to wrap my head around better. I&#39;m sure you guys have good thoughts on it. When we look at like an organism, right? And we look at its genome or we look at sort of proteins, like we&#39;re pretty confident we&#39;re talking about real things. I guess they&#39;re sort of maybe like quantumly fuzzy on some margin, but like we have a pretty good sense for a gene is a gene and a protein is a little machine. And here in the interpretability side, the features, if you will, that are identified or that are learned by NSAE or similar techniques, you might want to, if appropriate, separate SAEs from other techniques in your answer here. But these things seem to be sort of approximations or sort of there&#39;s some sort of gap right there between what is going on in a model and sort of what is going on when it&#39;s sort of sparsified In this sort of particular way. These features that are learned and the like labels that we give them and sort of how much correspondence you think there is there is that on a spectrum and how should we think about it

Tom McGrath
Yeah so it&#39;s interesting like one i suspect if you asked a biologist like they probably have a lot of corner cases about like is it really a gene um yeah biology, if nothing else, a great Supply of corner cases just because of the rich complexity of the world. But yes, I think there is a definite sense there that a gene is a natural abstraction. It&#39;s a good way to talk about the world. So this process of you take a model, you sparsify it, well now we have introduced some degree of lossiness, right? Because we&#39;re not capturing all of the computation. You can see this from the loss. You can see this from the reconstruction error. But we are capturing what look like very interesting and interpretable things. But that takes you on to the sort of next level of like, okay, there&#39;s a thing. There&#39;s like a feature in your sparse dictionary. And now we assign it a label. And this is another area for sort of a gap, this sort of gap that you can fall in. And I think that we can be in the business of closing both of these gaps, like a great deal. And you can close the gaps in multiple ways. One is to have a better idea. Oh, so the first gap, right? Gap one, where we&#39;re talking about the distance between the model and our approximation of the model. Then how do you close this gap? Well, one answer is you do the machine learning better. You just make a better essay. So you capture more of the loss. And there have been a bunch of this work in this direction. I can provide a bunch of papers later. Another is you try and answer the question, what does it mean for something to be a good abstraction? And you sort of use that as inspiration for new methods. So for instance, what would it mean for an SAA feature to be like a natural unit of computation? Well, it&#39;s not completely clear. I think there&#39;s actually some interesting, but probably quite resolvable issues there. One thing it might mean is that it&#39;s involving consistent computational parts. A feature is a natural unit of computation if it&#39;s sort of involved in other computations that make sense. And so now this is maybe a way, maybe one way of saying this is I think that things get a lot cleaner, like feel like they will get cleaner as we move to circuits rather than like just single Layer activations because you don&#39;t really have a great way of validating. Now you can validate by like intervening on a feature and seeing how things change. That&#39;s sort of like circuits except you just haven&#39;t tracked the circuit. Now you&#39;re sort of entering into the second part of the second gap. You know I&#39;ve intervened on a feature you might say well I intervened on it and it didn&#39;t do what I expected. But that might be because my expectation was wrong. That I&#39;ve simply fallen into the second gap where I&#39;ve given the feature an incorrect description. It is a unit of computation that the model does, but I&#39;ve just called it the wrong thing. Now, how do we narrow the second gap? And I think that the answer here is probably that we just get better at doing experiments on interpretability. So the way that we currently assign labels is, hopefully I won&#39;t offend Nick by saying this, is a little primitive. So Nick Kamratta, he invented this automated interpretability technique. And the principal investigator again fired. Yes. Thank you. I was like, any question that you&#39;re following? Yeah, he&#39;s wonderful. So anyway, hopefully I won&#39;t defend him by saying the current method of assigning semantics to features is a little primitive. And what we do is we give a frontier model a bunch of examples of where the feature fired. And we say, well, here are these examples. What&#39;s the feature? And this gets you some way, right? But it doesn&#39;t get you the whole way. For instance, if you were to ask me with access to the model what is this feature i wouldn&#39;t only do that i would also like try steering the feature and see what happens i might like look At other things that projected into that feature or like where it goes downstream you know how it relates to other features all that sort of thing so there are like many more things that I personally would do but we can&#39;t currently get models we can&#39;t currently get frontier models to do this it may just be a matter of scaffolding. We just need to build this kind of scaffolding such that they can use their capability set. There&#39;s a really run on odds. Yeah. I think there

> **Interpretability&#39;s Proto-Paradigm**

- Mechanistic interpretability is now proto-paradigmatic, not pre-paradigmatic.
- There is growing consensus features are linear directions forming circuits with superposition enabling more concepts than dimensions.

Transcript:
Tom McGrath
But I think we probably have kind of a proto paradigm in interpretability. And so we should push it. We can do a lot of useful stuff. We should keep pushing it. We should keep doing the useful stuff and wait for the anomalies to reveal themselves. Yeah, I was going to ask actually, are we still pre-paradigmatic? We&#39;ve upgraded ourselves now to proto-paradigmatic. I&#39;m going to say proto-paradigmatic. Maybe I should have some encouraging my convictions instead of it. I think we&#39;re like entering our first paradigmatic phase of interpretability. Well, no. Okay, so this is a bit fuzzy, right? Like, what is a paradigm? A paradigm is a sort of social thing. I don&#39;t think there&#39;s consensus. There&#39;s not the kind of consensus that would lead me to say there is a field-wide paradigm in interpretability. I would say that among a reasonably large group of people, there&#39;s the raw materials for a paradigm. I suspect if the field were anthropic,

> **Core Interpretability Paradigm Beliefs**

- Neural networks contain understandable features, often linearly decodable as vectors encoding intensity.
- Superposition allows models to store more concepts than dimensionality, with features forming computational circuits.

Transcript:
Tom McGrath
There are things to interpret. Interpretability is possible. And then what other parts of the paradigm are there? I suppose there&#39;s features, like representations are linearly decodable. That&#39;s, or at least like linear decoding is a reasonable way to talk about features. You should, there may be higher order structure, right? Things, you might have features that like features are sort of arrows in space. It might be that actually multiple features kind of lie on some manifold or in some subspace, but that it&#39;s a sensible way to talk about representations as lines through embedding space. Third part of this paradigm is this idea of superposition. Because if you&#39;re going to have vectors in a vector space, then the natural conclusion would be like, well, I&#39;m in a demodel sized space, right? Well, does that mean the model can only think of demodel things? Probably not, right? A language model can think of more than 4,096 things or something. So the other part of this paradigm is superposition, which is this idea that the way that you put, the way that you squash more of these embeddings, more of these vectors, these feature Vectors into the same space is by allowing them to overlap a little bit. And this creates a sort of bit of interference, bit of noise in the representations that models are still able to like deal with. Now, I guess another part of this is that magnitude, like direction along the vector constitutes intensity. And the other thing is rather obviously, I suppose, features connect to form circuits. And that is basically, I think, the paradigm, the paradigm which is currently, or rather the thing in which, this is like the anthropic paradigm, I would say. And if you were to blow this up to the size of the world, if there were consensus on this then I guess it has enough structure to be called paradigm Is there any competing proto-paradigm

Nathan Labenz
Or is there just
