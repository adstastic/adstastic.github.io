---
title: "I Got Fooled by AI-for-science Hype—here&#39;s What It Taught Me"
date: 2025-05-19
tags:
  - quote
ref: https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres?r=5lwff8&amp;utm_medium=ios&amp;triedRedirect=true
---
Quoting [Nick McGreivy](https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres?r=5lwff8&amp;utm_medium=ios&amp;triedRedirect=true):

> in 2023 DeepMind [claimed](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/) to have discovered 2.2 million crystal structures, [representing](https://www.nature.com/articles/s41586-023-06735-9) “an order-of-magnitude expansion in stable materials known to humanity.” But when [materials](https://x.com/Robert_Palgrave/status/1744383962913394758) [scientists](https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002) [analyzed these compounds](https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643), they found it was “[mostly junk](https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool)” and “respectfully” suggested that the paper “does not report any new materials.”

> Every field of science is experiencing AI differently, so we should be cautious about making generalizations. I’m convinced, however, that *some* of the lessons from my experience are broadly applicable across science:

•   AI adoption is exploding among scientists less because it benefits science and more [because it benefits the scientists themselves](https://arxiv.org/abs/2412.07727).
    
•   Because AI researchers almost never publish negative results, AI-for-science is experiencing [survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias).
    
•   The positive results that get published tend to be overly optimistic about AI’s potential.
    

As a result, I’ve come to believe that AI has generally been less successful and revolutionary in science than it appears to be.

> Most scientists aren’t trying to mislead anyone, but because they face strong incentives to present favorable results, there’s still a risk that you’ll be misled. Moving forward, I would have to be more skeptical, even (or perhaps especially) of high-impact papers with impressive results.

> people rarely publish papers about when AI methods fail, only when they succeed.

> Papers with large speedups *all* compared to weak baselines, suggesting that the more impressive the result, the more likely the paper had made an unfair comparison.

> ![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png)

> “weak baselines lead to overly positive results, while reporting biases lead to under-reporting of negative results.”

> even when AI achieves genuinely impressive results *in* science, that doesn’t mean that AI has done something useful *for* science. More often, it reflects only the *potential* of AI to be useful down the road.

This is because scientists working on AI (myself included) often work backwards. Instead of identifying a problem and then trying to find a solution, we start by assuming that AI will be the solution and then looking for problems to solve. But because it’s difficult to identify open scientific challenges that can be solved using AI, this “[hammer in search of a nail](https://x.com/MilesCranmer/status/1879542350541635882)” style of science means that researchers will often tackle problems which are suitable for using AI but which either have already been solved or don&#39;t create new scientific knowledge.

> pitfalls often cause the successful results that do get published to reach overly optimistic conclusions about AI in science. The details and [severity](https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool) seem to differ [between](https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002) [fields](https://x.com/Robert_Palgrave/status/1744383962913394758), but [pitfalls mostly](https://www.nature.com/articles/d41586-019-02307-y#ref-CR2) have fallen into one of [four categories](https://arxiv.org/abs/2407.12220): [data leakage](https://reproducible.cs.princeton.edu/), [weak](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184604) [baselines](https://x.com/tunguz/status/1853545690565058723), [cherry-picking](https://news.ycombinator.com/item?id=36231147), and [misreporting](https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643).

> I encourage people to treat impressive results in AI-for-science the same way we treat surprising results in nutrition science: with [instinctive](https://www.theatlantic.com/magazine/archive/2023/05/ice-cream-bad-for-you-health-study/673487/) [skepticism](https://www.cbsnews.com/news/how-the-chocolate-diet-hoax-fooled-millions/).