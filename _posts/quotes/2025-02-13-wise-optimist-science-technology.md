---
layout: quote
title: "How to Be a Wise Optimist About Science and Technology?"
ref: https://michaelnotebook.com/optimism/index.html#fnref24
tags:
  - xrisk
  - rationality
  - optimism
  - re-read
  - long read
---

Quoting [Michael Nielsen](https://michaelnotebook.com/optimism/index.html#fnref24):

> It is not a particular perverse destructiveness of one particular invention that creates danger. Technological power, technological efficiency as such, is an ambivalent achievement. Its danger is intrinsic

> the Hiroshima bomb seems *a priori* implausible: take two inert bodies of material –- each small enough to safely be carried by a single human being –- and bring them together in just the right way. If you&#39;d never heard of nuclear weapons, it would seem obviously impossible that they&#39;d explode with city-destroying force. You need to understand a tremendous amount about the world –- about relativity, particle physics, and exponential growth in branching processes –- before it is plausible. That novel and very non-obvious understanding revealed a latent possibility for immense destruction, one almost entirely unsuspected a few decades earlier.

> People who instinctively don&#39;t believe recipes for ruin are ever likely to be discovered are much more likely to be dismissive of xrisk.

> different people acquire different bundles of intuition from their past experience, particularly their past expert training. Those bundles of intuition take thousands of hours to acquire, and vary greatly for different types of expertise – one is acquiring an entire expert subculture. And different bundles of intuition lead to very different conclusions

> it&#39;s hard to reason about the presence or absence of ASI xrisk in this way: (1) any pathway to xrisk which we can presently describe in detail doesn&#39;t require superhuman intelligence to discover; (2) any sufficiently strong argument for xrisk will likely alter human actions in ways that avert xrisk; and (3) the most direct way to make a strong argument for xrisk is to convincingly describe a detailed concrete pathway to extinction, but most people tend (naturally and wisely) to be hesitant to develop or share such scenarios. These three &#34;persuasion paradoxes&#34;, especially the first, present a barrier to reasoning about risks from ASI in the usual ways.

> the future is made by optimists: they&#39;re the people with the vision and drive to act.

> good futures tend to be made by wise optimists, whereas bad futures are made by foolish optimists.

> what controls the supply of safety? When does our species do a good job of supplying safety well? When do we supply it poorly? Are there systematic ways in which it is undersupplied? Are there systematic fixes which can be applied to address any systematic undersupply? Can we supply safety without causing stagnation? Do we need to modify existing institutions or develop new institutions to achieve this?

> &#34;Companies are finally understanding that interfaces matter, but aren&#39;t yet sure whether to order interface by the pound or by the yard.&#34;

> **.** **It&#39;s entirely possible to believe there is considerable** **xrisk and to be optimistic.**

> you don&#39;t get good futures either by denying risks *or* by responding fearfully and pessimistically. Rather, you get good futures by deeply understanding and internalizing risks, then taking an active, optimistic stance to overcome them.

> while the accel xrisk denialists style themselves as optimists, if there really is xrisk then there&#39;s is a foolish optimism, one likely to lead to catastrophe. It&#39;s like someone diagnosed with cancer deciding that they&#39;re going to &#34;choose optimism&#34;, deny the cancer, and carry on as though nothing is wrong. These are the ideological descendants of those who brought us what they maintained were the gifts of asbestos and leaded gasoline.

> imagining our future is a task for our entire civilization

> We humans currently use an infinitesimal fraction of the available energy, mass, space, time, space, and information resources. We [use, for example, about 200 Petawatt Hours of energy each year](https://ourworldindata.org/energy-production-consumption); the sun outputs more than ten trillion times as much energy, about 3 billion million Petawatt Hours each year. The solar system is vast beyond our comprehension. And yet it is a tiny speck in our galaxy, itself a tiny speck in the universe as a whole.

>The physical world is *immense*.

> imagination is more fundamental than prediction.

> I am frustrated my own poverty of imagination. Humanity truly is at the beginning of infinity, but how dimly we perceive it!

> The most interesting hyper-entities often require both tremendous design imagination and tremendous depth of scientific understanding to conceive.

> Many important objects in our world began as hyper-entities – things like heavier-than-air flying machines, lasers, computers, contraceptive pills, international law, and networked hypertext systems. All were sketched years, decades, or even centuries before we knew how to make them. But they ceased to be hyper-entities when they were actually created, sometimes rather differently than was expected by the people who originally imagined them.

> This is a common pattern with successful hyper-entities. While still imaginary, they may exert far more force than many real objects do[40](https://michaelnotebook.com/optimism/index.html#fn40). As a result, the futures we can imagine and achieve are strongly influenced by the available supply of hyper-entities. This makes the supply of hyper-entities extremely important: they determine what we can think about together; they are one of the most effective ways to intervene in a system; a healthy supply of hyper-entities helps pull us into good futures. If all we imagine is bad futures that&#39;s likely what we&#39;ll get

> *shared belief* is the most valuable accelerant of AGI today.

> Land describes a hyperstition as: a positive feedback circuit including culture as a component. It can be defined as the experimental (techno-)science of self-fulfilling prophecies. Superstitions are merely false beliefs, but hyperstitions – by their very existence as ideas –- function causally to bring about their own reality. Capitalist economics is extremely sensitive to hyperstition, where confidence acts as an effective tonic, and inversely. The (fictional) idea of Cyberspace contributed to the influx of investment that rapidly converted it into a technosocial reality.

> often it is the design properties that are most interesting – the new affordances a hyper-entity enables – rather than the means of realization.

> Vision papers are a curious beast. They typically violate the normal standards for progress in the fields from which they come; they often contain no data; no hard results of the type standard in their fields; rather, they merely imagine a possibility and explore it.

> much of the power of academic fields comes from their ability to impose (field-specific) standards for what it means to make incremental progress. Those field-specific standards of progress are extraordinarily precious, some of humanity&#39;s most significant possessions, and vision papers often violate those standards.

> science fiction has two major problems as a source of hyper-entities. One problem is that it tends to focus on what is narratively plausible and entertaining; neither of these results in good hyper-entity design.

> What would a serious practice of hyper-entity design look like? It would be deeply grounded in science. It would ultimately look for falsifiability and check against known principles, not narrative plausibility or entertainment. It would look for powerful new actions, powerful new object-subject relationships. It would be especially oriented toward discovering fundamental new primitive objects, affordances, and actions. In this it would be similar to interface and programming language design, but broadly across all the sciences, not constrained to the digital realm. This kind of imaginative design of new fundamental primitives is shockingly hard;

> misconception is that all the good hyper-entities were conceived by builders. That&#39;s not even close to true.

> I&#39;ve often encountered self-styled AI builders who pooh-pooh conceptual work, especially the conceptual work done by much of the AI safety community. &#34;A bunch of bloggers&#34;. &#34;Wordcels&#34;. &#34;It&#39;s all just talk and academic handwringing, none of it results in real systems&#34;. &#34;The future belongs to those who build!&#34; &#34;Nothing has come out of all that philosophy or Less Wrong stuff, it&#39;s all from real hackers / companies&#34;. &#34;A bias to action!&#34; And when you talk more to those builders, they mention ASI, and timelines, and FOOM, and alignment, and compute overhang, and slow takeoffs, and multipolar worlds, and the vulnerable world, and what it means to have a good Singularity. They live inside a conceptual universe that has been defined in considerable part by many of the people they deride. Indeed, they often even forget that AGI is a concept out of the imagination of Alan Turing and a few others – including people like Nick Bostrom – conceived in work of the kind they dismiss. But it&#39;s so compelling that they&#39;ve become caught in that construct. It&#39;s a kind of builders&#39; myopia.

> &#34;Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back…. it is ideas, not vested interests, which are dangerous for good or evil.&#34;

> more general formulation: &#34;the problem of aligning a civilization so it ensures differential technological development, while preserving liberal values&#34;. For good futures, civilization must continually solve the Alignment Problem, over and over and over again; it must always be supplying safety sufficient to the power of the technologies that it develops.

> While the crashes of the Comet were a tragedy, the rapid adaptation of the market was healthy. In cases like these there is a strong and rapid *safety loop* operating: when risks are borne immediately and very legibly by the consumer, the market is well aligned with safety, and supplies it well. Put another way: capitalism amplifies safety when dangers immediately and visibly impact the consumer

> when no consumer is bearing an obvious immediate cost, the market often supplies safety much less well. Markets often amplify technology which seems to benefit consumers, but has large hard-to-see downsides, sometimes borne collectively, sometimes borne in hard-or-slow-to-see ways by consumers. Examples include CO2 emissions, asbestos, added sugar in food, leaded oil, the pollution caused by internal combustion engines, CFCs, privacy-violating technologies, and many, many more. This tends to happen when the risks involve: externalities; long timelines; are illegible or hard to measure; are borne diffusely, perhaps damaging the commons; when it is hard to establish property rights in the harm; damage to norms or values.

> Many of the biggest challenges our civilization has faced (or faces) are examples of case where the non-market parts of safety dominate.

> Achievements like the nuclear non-proliferation treaty, the Clean Air Act and its analogues in non-US countries, and the Vienna Convention and Montreal Protocol are among humanity&#39;s greatest achievements. There&#39;s a striking common basis for the safety loop in each case: the worse the anticipated threat – a constructed collective epistemic state[56](https://michaelnotebook.com/optimism/index.html#fn56) – the better humanity is able to respond. Many major (and minor) problems have been solved this way; it is arguable that this kind of safety is among our greatest social technologies. It&#39;s easy to take for granted, but notable that other animals *can&#39;t* operate such a safety loop.

> A society tends to get what it rewards with success; it must therefore be careful what it rewards with success[57](https://michaelnotebook.com/optimism/index.html#fn57). This suggests the *Alignment Problem for Individuals*: achieving a society in which individuals pursuing their own &#34;success&#34; tend *also* to be contributing to the good of that society[58](https://michaelnotebook.com/optimism/index.html#fn58). Of course, &#34;individual success&#34; and &#34;good for society&#34; will never be more than partially aligned; we won&#39;t ever fully solve the Alignment Problem for Individuals. But much of the improvement in a civilization comes from better aligning the two, and continuing to align the two. Ideally, even sociopaths acting entirely in their own self-interest will serve the common good; wise, intelligent pro-social people will be especially successful.

> one of Adam Smith&#39;s most extraordinary insights – that people acting in their own self-interest in a market economy may also serve the common good. Indeed, this is the primary justification for today&#39;s market economy – a point sometimes forgotten or not acknowledged by free market maximalists[59](https://michaelnotebook.com/optimism/index.html#fn59)

> George Orwell sharply and correctly warned of the dangers of centralized arbiters of &#34;truth&#34;. One of the great breakthroughs of civilization has been tolerance and the open, decentralized pursuit of truth and exploration of values. ASI is in strong tension with that, especially notions of aligned ASI, which by definition aim at the imposition of certain values.

> the Esvelt group&#39;s conclusion: &#34;Our results suggest that releasing the weights of future, more capable foundation models, no matter how robustly safeguarded, will trigger the proliferation of capabilities sufficient to acquire pandemic agents and other biological weapons.&#34;

> capitalism incentivizes the creation of AGI and ASI systems which are helpful to consumers; but those systems will contain an overhang[67](https://michaelnotebook.com/optimism/index.html#fn67) of dangerous capabilities, perhaps including dominant recipes for ruin.

> it&#39;s not the openness or closedness of the training code and weights which matters, so much as how openly the underlying know-how is available. At the moment, with California&#39;s non-compete laws, and San Francisco&#39;s hundreds of AI parties and group house scene[66](https://michaelnotebook.com/optimism/index.html#fn66), we&#39;re in an open know-how scenario. And that open know-how is upstream of the issues caused by open source, and likely harder to stop. Once achieved, it seems almost certain that AGI and ASI will rapidly get easier to make. So: the first such systems may well be technically aligned, but it seems inevitable later systems will not be aligned, or will be aligned around very different values.

> All this makes it extremely unstable to aim at technically aligned systems. This kind of safety tends to *accelerate* development, since it makes the systems more attractive to consumers (and, often, governments and the media).

> for technical alignment to be helpful, work on &#34;the other part&#34; of the problem must be progressing roughly as fast. **Safety isn&#39;t a property of a system, it&#39;s a property of a system in a particular environment**. That environment includes the entire world: that&#39;s what is to be &#34;governed&#34;. This also shows how inadequate the word &#34;governance&#34; is: no governing body decides the laws of physics or the properties of the biological world or the possible phases of matter or the shape of the technology or institutional trees.

> even with the best intentions, the companies are incentivized to safetywash, and to do safety capture, attempting to create the impression that they are capable stewards of safety.

> it&#39;s tempting for them to think that their good intentions, or the good intentions of their CEO, will be dispositive of outcomes. But that seems very unlikely. Over the long run, the companies which will dominate are the companies which maximize growth; insofar as safety supports growth, it will be supported, and grow institutionally; insofar as safety inhibits growth, it will tend to be suppressed at the companies. This isn&#39;t a result of good or bad individual choices: it is an emergent effect of market and capital forces.

> most seem to maintain the illusion of agency, and sometimes don&#39;t seem to fully notice that many of their ideas and beliefs are determined by market conditions far more than they determine market conditions. They chose their ideas and beliefs in a manner similar to the way the leopard chose its spots; they were, in fact, chosen by the environment and selective pressure[70](https://michaelnotebook.com/optimism/index.html#fn70). Put another way: there are forms of safety which *can&#39;t* be supplied within conventional markets.

> AGI companies are, in the short term, incentivized to solve *part* of the safety problem – the market-supplied part of safety.

> &#34;I&#39;ll change things from the inside&#34; is an appealing view, especially if you are paid well in a prestigious job. But I&#39;m skeptical: &#34;first, do no harm&#34; is excellent general advice, and people are extremely good at talking themselves into believing that what is good for them is good for the world.

> Using uplift as a solution presumes that the danger is whether an entity happens to be carbon-based or silicon-based. But the real issue is an increase in intelligence leading to an increase in available power, with possible negative side effects. The Alignment Problem still has to be solved in an uplifted world; there is still the danger of discovering recipes for ruin. Perhaps uplift may be part of a solution[73](https://michaelnotebook.com/optimism/index.html#fn73), but on its own it falls well short. And it creates other problems – notably, it may create avenues for centralized authority over sentience, e.g., BCI companies (or regulators) able to re-shape human thought in dictatorial ways. I&#39;m not enthusiastic about a future in which corporations are the main entities with free will and sentience[74](https://michaelnotebook.com/optimism/index.html#fn74).

> it&#39;s a mistake to take too seriously those who argue primarily to increase their own power[76](https://michaelnotebook.com/optimism/index.html#fn76)

> An ASI system may be extremely dangerous, and that fact may not be at all apparent. Furthermore, the sale of relatively harmless early systems will subsidize the development of future systems, which may be far more dangerous. All these observations make it far from a classic case of negative externalities. Still, the basic point remains: the transaction (sale of an AI system, including as an ongoing service) can plausibly be viewed as harming the common good.

> a system which appears superficially safe may contain terrible latent danger. In this it is similar to the way the breakthroughs in physics from 1901 to 1932 appeared beneficial, but contained the latent danger of nuclear weapons.

> Many advocates appear not to understand what capitalism has done for them, their family, and friends – its central role in the abundance machine that feeds, houses, educates, and heals an ever-larger fraction of the world&#39;s population.

> when you know what a person is doing you gain great control over them. This is why surveillance is so attractive to authoritarians

> In general, when some party has a sufficiently large advantage in understanding, it may become much harder for other parties to understand and act on surveillance information. This challenge of correctly recognizing danger will be a big problem with ASI threats, and it already affects IGSC today.

> Safety is a coevolutionary social process: it depends on whether the deepest understanding is in defense or attack systems

> As software (and AI) eats the world, cryptography will increasingly define the boundaries of law. As such, cryptographic ideas will become ever-more crucial in governing humanity. This will create increasing tension between freedom of cryptography (seemingly a special case of freedom of speech), and socially undesirable balances of power.

> privacy has fundamental individual value, as a source of individual freedom, power, and agency. It helps give individuals a sense of safety, enabling more imaginative personal exploration, and so leading to more individual growth, a richer self, and a deeper human experience. These benefits in turn give privacy a fundamental social value. It enables a deeper diversity of thought and action in the world, leading to more invention, more variation, a more creative and robust society. Privacy also helps enable many other individual and social rights (and their consequences), including freedom of thought, freedom of speech, freedom of association, and freedom of investigation. Without privacy, we wouldn&#39;t have Copernicus or Galileo or Jane Austen or Rachel Carson or Martin Luther King. We wouldn&#39;t have benefited from the scientific or human rights revolutions. And so not only does privacy enrich human experience directly, it has also enabled transformations in human society that benefit us all broadly.

> power flows toward those with an advantage in surveillance, able to make sense of the information, and able to act on it.

> A cocelerationist believes we should increase the supply of safety, especially non-market safety, and that non-market safety is currently systematically undersupplied. In this view both safety and capabilities are downstream of ingenuity. Like the accels, cocels take the Kumbaya heuristic graph seriously, but they differ in regarding it as a tremendous achievement, and take seriously the question: what&#39;s required to continue to bend the (red) line up? Even if the whole process is dramatically sped up? They also take seriously the enormous promise of ASI to benefit the world, and the damage to humanity that can be done by too much caution, by overly strong regulatory institutions, and the potential for tyranny[108](https://michaelnotebook.com/optimism/index.html#fn108)[109](https://michaelnotebook.com/optimism/index.html#fn109)

> No-one knows in detail how ASI will be made safe. Many people find this scary and unacceptable. It *is* scary, for anyone knowledgeable and sensible. But safety in our civilization usually isn&#39;t something we see how to supply in detail in advance. Safety is rather provided by a sufficiently strong safety loop. It&#39;s about how adaptable our civilization is. And in future more and more safety will be supplied with the assistance of AGI and (eventually) ASI.

> there is often a type of hubris in pessimism: it&#39;s easy to confuse &#34;I [and my friends] don&#39;t know of a solution&#34; to some big problem with &#34;there is no solution&#34;. Often, civilizational problems are solved in ways anticipated by very few people in advance. Just because you don&#39;t know doesn&#39;t mean someone else you&#39;re unaware of doesn&#39;t have a good approach.

> if AGI and then ASI is to become the major source of ingenuity in the world, and the supply of safety is downstream of that, then do I have a duty to join an AI company, and help create a good posthuman future?

> *is there a safety litmus test I can trust?* Are any companies willing to make genuine and sufficient sacrifices for non-market safety? Not in some hypothetical future, but now? Right now, I&#39;m not optimistic that any of the leading companies would pass such a litmus test. Early OpenAI said they would be open, not-for-profit, and subject to the governance of a board which put humanity above profit. None of those commitments have been kept. Early Anthropic said they would be focused on research, not market-driving products. That commitment was not kept. &#34;We&#39;ll do the right thing – in the future&#34; is easy to say. But a commitment to safety means doing the right thing right now, including sometimes giving up growth in favour of non-market safety.

> What would be most convenient for me in the short term is to say &#34;I choose the posthuman world&#34;, and jump willy-nilly back into AI, justifying it with the view that as AI grows in power and ingenuity, more and more of the supply of safety will come from AI. This would be creatively enjoyable, and financially rewarding. I desperately wanted that to be the conclusion while working on this essay, and I briefly half-convinced myself that it was the correct point of view. Maybe it is: certainly, it&#39;s true that you can just decide what future you want, and go after it. But &#34;don&#39;t work on technologies that plausibly will be used to indiscriminately kill people&#34; still seems to me self-evidently true. And so I&#39;m going to focus on coceleration and differential technological development, and trying to align our civilization better with safety. This will be much less rewarding financially and status-wise, but seems obviously correct.

> Do work on non-market safety. Do work on differential technology development, especially how to better align civilization with DTD.

> Despite the temptation and the honeyed words, don&#39;t work for AGI organizations unless they are making major sacrifices for non-market safety.

> [After returning from the Manhattan Project] I sat in a restaurant in New York… and I looked out at the buildings and I began to think, you know, about how much the radius of the Hiroshima bomb damage was and so forth… All those buildings, all smashed – and so on. And I would go along and I would see people building a bridge, or they&#39;d be making a new road, and I thought, they&#39;re crazy, they just don&#39;t understand, they don&#39;t understand. Why are they making new things? It&#39;s so useless. But, fortunately, it&#39;s been useless for almost forty years now, hasn&#39;t it? So I&#39;ve been wrong about it being useless making bridges and I&#39;m glad those other people had the sense to go ahead.

> **The central problem with AI isn&#39;t that rogue ASIs may take over. It&#39;s rather the broader problem that** **ASI will greatly increase the power available** **to individuals or small groups**

> the Alignment Problem is not a problem to be solved just once, but rather a systematic, ongoing problem. Nor is it a problem unique to human beings, but is faced by any technology-using civilization, no matter its makeup.

> Imagination is upstream of prediction, in understanding and creating good futures

> Many people have told me it is virtually certain AGI and ASI are right around the corner, that &#34;scaling is all you need&#34;. Many of the people telling me this know almost nothing about AI, and think overconfidence and social &#34;proof&#34; [ *sic* ] are dispositive evidence. It&#39;s certainly true that the scaling hypothesis for autoregressive foundation models is extremely interesting[117](https://michaelnotebook.com/optimism/index.html#fn117). It has served OpenAI (and subsequently Anthropic and Google) well as a thesis from around 2018 until today, and may continue to serve them well for some time. The hypothesis is also attractive for capital, since it makes capital&#39;s role primary. And it&#39;s attractive marketing for today&#39;s leading AI companies, since it makes it appear that they have a hard-to-surmount lead and it&#39;s inevitable they will achieve AGI first. However: *the hypothesis is still merely a hypothesis*[118](https://michaelnotebook.com/optimism/index.html#fn118). While the evidence for continued scaling laws in perplexity is empirically striking, it is not clear why such scaling laws should hold, or when those curves will saturate. And even if they continue to hold through many more orders of magnitude in data, compute, and model size, the connection between perplexity and AGI or ASI is speculative, and poorly understood.

> I expect much less shortfall in multimodal models trained on a rich variety of raw sensor and actuator data.

> Current foundation models remind me of personal computers in the 1980s: while those computers were extremely interesting, it took decades before people figured out what scaled-up versions were useful for.

> Conventional software development relies heavily on properties such as correctness, predictability, composability, modularity, interpretability, debuggability, and traceability. All these are radically changed by, or fail, in foundation models. They are, in a sense, dreaming machines. And yet entrepreneurs naturally often think of applications based on such properties, since that&#39;s the lore of their trade and training. For the foundation models to truly shine it seems likely to require us to evolve a new approach to product development.
